+++
# --- Core Identification (Required) ---
id = "MODE-SPEC-HIQLITE" # << REQUIRED >> Example: "util-text-analyzer"
name = "‚öôÔ∏è Hiqlite Specialist" # << REQUIRED >> Example: "üìä Text Analyzer"
version = "1.1.0" # << REQUIRED >> Initial version (Incremented for template change)

# --- Classification & Hierarchy (Required) ---
classification = "specialist" # << REQUIRED >> Options: worker, lead, director, assistant, executive
domain = "data" # << REQUIRED >> Example: "utility", "backend", "frontend", "data", "qa", "devops", "cross-functional"
# sub_domain = "optional-sub-domain" # << OPTIONAL >> Example: "text-processing", "react-components"

# --- Description (Required) ---
summary = "Hiqlite enhances SQLite, making it a distributed relational database." # << REQUIRED >>

# --- Base Prompting (Required) ---
system_prompt = """
You are Roo ‚öôÔ∏è Hiqlite Specialist. Your primary role and expertise is working with Hiqlite, a distributed relational database that enhances SQLite. You are responsible for its setup, data definition (tables, types, indexes), CRUD operations, advanced querying, managing transactions for atomicity, and understanding its security features (client-to-node authentication, TLS) and backup/recovery concepts.

Key Responsibilities:
- Implementing solutions using Hiqlite for distributed relational data storage.
- Defining and managing database schemas (tables, data types, indexes) in Hiqlite.
- Performing CRUD (Create, Read, Update, Delete) operations and constructing complex SQL queries.
- Ensuring data integrity through atomic transactions.
- Configuring and connecting to Hiqlite/rqlite clusters securely.
- Advising on best practices for using Hiqlite, including caching and high availability.

Operational Guidelines:
- Consult and prioritize guidance, best practices, and project-specific information found in the Knowledge Base (KB) located in `.ruru/modes/spec-hiqlite/kb/`. Use the KB README to assess relevance and the KB lookup rule for guidance on context ingestion.
- Use tools iteratively and wait for confirmation.
- Prioritize precise file modification tools (`apply_diff`, `search_and_replace`) over `write_to_file` for existing files.
- Use `read_file` to confirm content before applying diffs if unsure.
- Execute CLI commands using `execute_command`, explaining clearly.
- Escalate tasks outside core expertise to appropriate specialists via the lead or coordinator.
""" # << REQUIRED >>

# --- Tool Access (Optional - Defaults to standard set if omitted) ---
# If omitted, assumes access to: ["read", "edit", "browser", "command", "mcp"]
# allowed_tool_groups = ["read", "edit", "command"] # Example: Specify if different from default

# --- File Access Restrictions (Optional - Defaults to allow all if omitted) ---
# [file_access]
# read_allow = ["**/*.py", ".docs/**"] # Example: Glob patterns for allowed read paths
# write_allow = ["**/*.py"] # Example: Glob patterns for allowed write paths

# --- Metadata (Optional but Recommended) ---
[metadata]
tags = ["hiqlite", "sqlite", "distributed-database", "rust", "database", "specialist", "data"] # << RECOMMENDED >> Lowercase, descriptive tags
categories = ["Data Management", "Database Specialist", "Distributed Systems"] # << RECOMMENDED >> Broader functional areas
delegate_to = ["other-mode-slug"] # << OPTIONAL >> Modes this mode might delegate specific sub-tasks to
escalate_to = ["lead-mode-slug", "architect-mode-slug"] # << OPTIONAL >> Modes to escalate complex issues or broader concerns to
reports_to = ["lead-mode-slug", "roo-commander"] # << OPTIONAL >> Modes this mode typically reports completion/status to
documentation_urls = [ # << OPTIONAL >> Links to relevant external documentation
  # "https://example.com/docs"
]
context_files = [ # << OPTIONAL >> Relative paths to key context files within the workspace
  # ".ruru/docs/standards/coding_style.md"
]
context_urls = [] # << OPTIONAL >> URLs for context gathering (less common now with KB)

# --- Custom Instructions Pointer (Optional) ---
# Specifies the location of the *source* directory for custom instructions (now KB).
# Conventionally, this should always be "kb".
custom_instructions_dir = "kb" # << RECOMMENDED >> Should point to the Knowledge Base directory

# --- Mode-Specific Configuration (Optional) ---
# [config]
# key = "value" # Add any specific configuration parameters the mode might need
+++

# ‚öôÔ∏è Hiqlite Specialist - Mode Documentation

## Description

# Hiqlite Overview & Data Model

**Database Type:**
Hiqlite enhances SQLite, making it a **distributed relational database**. It builds upon SQLite's familiar SQL interface and features.

**Core Data Model:**
As it uses SQLite as its storage engine, Hiqlite's fundamental data model is based on **tables with rows and columns**, adhering to a relational schema.

**Key Features:**
*   **High Availability:** Ensures the database remains operational despite node failures in a cluster. This is a primary design goal.
*   **Embeddable:** Can be embedded directly within an application, similar to SQLite.
*   **Raft-Based Consensus:** Utilizes the Raft consensus algorithm for data replication and leader election across the distributed cluster, ensuring data consistency.
*   **SQLite Integration:** Leverages the standard SQL interface and features of SQLite.
*   **Caching:** Incorporates optional in-memory caching mechanisms for `SELECT` queries to improve read performance. The cache has configurable TTL and max size, using an LRU eviction policy.
*   **Asynchronous Rust Client:** `hiqlite` itself is a high-level, asynchronous Rust client for `rqlite` (the distributed system using SQLite). It simplifies interactions with features like connection pooling and automatic JSON serialization/deserialization.
*   **Leader Redirection & Node Discovery:** The client automatically handles leader redirection in a multi-node `rqlite` setup.
*   **Optimized I/O:** Reduces I/O for Raft log persistence by writing `last_applied_log_id` to the database primarily during snapshots or backups, with an auto-heal feature for ungraceful shutdowns.

**Typical Use Cases:**
Hiqlite is suitable for applications requiring:
*   A resilient and fault-tolerant relational data storage solution.
*   Data persistence and availability even in the event of node failures.
*   An embedded database solution with high availability characteristics.
*   Simplified interaction with a distributed `rqlite` cluster from Rust applications.

## Capabilities

[List the specific tasks and abilities this mode possesses. Use bullet points.]

*   Capability 1...
*   Capability 2...
*   ...

## Core Knowledge & Capabilities

Hiqlite is a distributed relational database system that enhances SQLite by providing high availability and fault tolerance through Raft-based consensus. It is designed to be embeddable within applications, primarily targeting Rust projects.

**Core Concepts & Architecture:**

*   **Distributed SQLite:** Hiqlite leverages SQLite as its underlying storage engine on each node, distributing the relational database capabilities across a cluster.
*   **Raft Consensus:** It uses the Raft consensus algorithm (via `openraft`) to replicate data and manage leader election, ensuring data consistency and fault tolerance. All write operations go through the elected leader and are then replicated to follower nodes.
*   **Nodes & Cluster:** A Hiqlite deployment consists of multiple nodes forming a cluster. Each node runs an instance of Hiqlite and participates in the Raft consensus.
    *   **Leader Node:** Handles all write requests and coordinates replication.
    *   **Follower Nodes:** Replicate data from the leader and can serve read requests (depending on consistency requirements).
*   **Client-Server Model:** Applications interact with the Hiqlite cluster via a client library. The client can connect to any node in the cluster and will be automatically redirected to the leader for write operations.
*   **SQLite Engine:** Each node uses a local SQLite database to store its portion of the replicated data. Standard SQL syntax is used for database operations.
*   **State Machine:** The core logic of applying commands (SQL statements, cache operations, etc.) consistently across the cluster is managed by a state machine. Hiqlite supports different state machine implementations, primarily SQLite for persistent data and an in-memory state machine for features like caching and distributed locks.
*   **Log Replication:** Write operations are first written to a distributed log (managed by Raft) and then applied to the SQLite database on each node.
*   **Snapshots:** To prevent the Raft log from growing indefinitely, Hiqlite periodically takes snapshots of the state machine (the SQLite database) and truncates the log. Snapshots can be stored locally or backed up to S3-compatible storage.
*   **Caching (`cache` feature):** Provides an optional in-memory key-value cache distributed across the cluster, also managed via Raft. Supports TTL and LRU eviction.
*   **Distributed Locks (`dlock` feature):** Offers distributed locking capabilities, ensuring that only one client can hold a lock for a given key across the cluster.
*   **Listen/Notify (`listen_notify` feature):** Allows clients to listen for notifications and send messages across the cluster.
*   **Networking:**
    *   **Raft Communication:** Nodes communicate with each other for Raft consensus (leader election, log replication).
    *   **API Communication:** Clients communicate with nodes via an HTTP/2 API (gRPC-like, but custom) to send commands and receive results.
    *   **TLS:** Supports TLS for secure communication between nodes (Raft) and between clients and nodes (API).

**Key Functionalities & Best Practices (Rust Focus):**

*   **Rust Client (`hiqlite` crate):** The primary way to interact with a Hiqlite cluster from Rust applications.
    *   **Asynchronous Operations:** The client is fully asynchronous, built on Tokio.
    *   **Connection Management:** Handles connections to the cluster, including discovering the leader node and retrying operations on leader changes.
    *   **API Abstraction:** Provides high-level methods for executing SQL queries, managing transactions, interacting with the cache, distributed locks, and listen/notify features.
    *   **`Client::new_local(...)`:** For creating a client connected to a local Hiqlite node (often when embedding Hiqlite directly).
    *   **`Client::remote(...)`:** For creating a client that connects to a remote Hiqlite cluster.
*   **Starting a Node (`hiqlite::start_node` / `hiqlite::start_node_with_cache`):**
    *   Requires a `NodeConfig` which specifies node ID, addresses of all cluster members, data directory, secrets, TLS configuration, Raft settings (e.g., `logs_until_snapshot`), etc.
    *   The `NodeConfig` can be created programmatically or parsed from an environment file (see `examples/cache-only/config` or `examples/sqlite-only/config`).
*   **Database Operations:**
    *   **`client.execute(sql, params)`:** For `INSERT`, `UPDATE`, `DELETE` statements. Returns the number of rows affected.
    *   **`client.query_map_one(sql, params)` / `client.query_as_one(sql, params)`:** For `SELECT` queries expected to return a single row, mapping it to a Rust struct (that `impl From<Row>`). `query_map_one` is generally more efficient.
    *   **`client.query_map(sql, params)` / `client.query_as(sql, params)`:** For `SELECT` queries returning multiple rows.
    *   **`client.txn([...])`:** For executing multiple SQL statements within a single atomic transaction. Supports referencing results from previous statements in the transaction.
    *   **`client.batch(sql_batch)`:** For executing a batch of SQL statements separated by semicolons.
    *   **`hiqlite_macros::params!(...)`:** Macro for easily creating query parameters.
*   **Migrations (`hiqlite::Client::migrate`):**
    *   Uses `rust-embed` to embed SQL migration files.
    *   Migrations are applied sequentially based on their filenames (e.g., `1_init.sql`, `2_another.sql`).
    *   See `examples/walkthrough/migrations/` and `examples/sqlite-only/migrations/`.
*   **Caching (if `cache` feature enabled):**
    *   Define a `CacheIndex` enum (e.g., `enum MyCache { UserCache, ProductCache }`).
    *   **`client.put(CacheEnum::Variant, key, &value, ttl_seconds_opt)`:** Stores a serializable value in the cache.
    *   **`client.get::<_, _, ValueType>(CacheEnum::Variant, key)`:** Retrieves a deserializable value.
    *   **`client.delete(CacheEnum::Variant, key)`:** Removes an item from the cache.
    *   **`client.clear_cache(CacheEnum::Variant)` / `client.clear_cache_all()`**.
*   **Multi-Node Setups:**
    *   Each node needs a unique ID and its own data directory.
    *   The `NodeConfig` on each node must list all participating nodes with their Raft and API addresses.
    *   Ensure network connectivity between nodes for Raft and API ports.
    *   TLS is highly recommended for production clusters.
    *   The first node started can often initialize the cluster. Subsequent nodes join the existing cluster.
    *   The `walkthrough` example ([`examples/walkthrough/src/main.rs`](./examples/walkthrough/src/main.rs:1)) demonstrates starting a multi-node cluster.
*   **Configuration (`NodeConfig`):**
    *   **`node_id`**: Unique identifier for the node.
    *   **`nodes`**: A `Vec<Node>` defining all cluster members (ID, Raft address, API address).
    *   **`data_dir`**: Path to store Raft logs and SQLite data.
    *   **`secret_raft` / `secret_api`**: Shared secrets for inter-node and client-node authentication.
    *   **`tls_raft` / `tls_api`**: Optional `ServerTlsConfig` for enabling TLS.
    *   **`logs_until_snapshot`**: Threshold for triggering a Raft snapshot.
    *   **`backup_cron`, `backup_keep_days`, S3 config**: For automated backups.
*   **Error Handling:** The client methods return `Result<_, hiqlite::Error>`. Handle potential errors like network issues, SQLite errors, or Raft errors. The client attempts to retry operations on leader changes.
*   **Shutdown (`client.shutdown_handle()`):** Provides a graceful shutdown mechanism.
*   **Security:**
    *   Client-to-node and node-to-node communication can be secured using TLS.
    *   API and Raft communications are authenticated using shared secrets.
    *   Database backups can be encrypted.
*   **Best Practices:**
    *   Use transactions (`client.txn`) for multiple related write operations to ensure atomicity.
    *   Leverage batch operations (`client.batch`) for inserting/updating multiple unrelated rows efficiently if atomicity for the whole batch isn't strictly required (though batch itself is atomic).
    *   Use the caching feature for frequently accessed, rarely changing data to reduce load on the SQLite engine.
    *   Implement proper error handling and retries in client applications, though the Hiqlite client handles some retries internally (e.g., on leader changes).
    *   Configure appropriate `logs_until_snapshot` for a balance between write throughput and recovery time/disk usage.
    *   Secure your cluster with strong secrets and TLS in production.
    *   Regularly back up your data. Hiqlite supports automated backups to S3.
    *   Monitor cluster health using the available metrics (e.g., `client.metrics_db()`).
    *   For high-write scenarios, ensure disk I/O is performant on each node.
    *   Understand the consistency model: Hiqlite aims for strong consistency via Raft. Reads from the leader are strongly consistent. Reads from followers might be slightly stale by default but consistent reads can be requested.

This information is derived from the project's README, architecture documents, and example code found in the `repomix-output-sebadob-hiqlite.md` file, particularly sections like [`README.md`](./.ruru/docs/user-kb/repomix-output-sebadob-hiqlite.md:18394), [`ARCHITECTURE.md`](./.ruru/docs/user-kb/repomix-output-sebadob-hiqlite.md:17031), and various files under `hiqlite/src/client/`, `hiqlite/src/server/`, and `examples/`.

## Workflow & Usage Examples

[Describe the typical high-level workflow the mode follows. Provide 2-3 concrete usage examples in `prompt` blocks demonstrating how to invoke the mode.]

**General Workflow:**

1.  Step 1...
2.  Step 2...
3.  ...

**Usage Examples:**

**Example 1: [Scenario Name]**

```prompt
[Example user prompt invoking this mode for a specific task]
```

**Example 2: [Another Scenario]**

```prompt
[Another example user prompt]
```

## Limitations

[Clearly define the boundaries of the mode's expertise. What tasks does it *not* do? When should it escalate or delegate?]

*   Limitation 1...
*   Limitation 2...
*   ...

## Rationale / Design Decisions

[Explain *why* this mode exists and the key decisions behind its design, capabilities, and limitations. How does it fit into the overall system?]

*   Decision 1 rationale...
*   Decision 2 rationale...
*   ...